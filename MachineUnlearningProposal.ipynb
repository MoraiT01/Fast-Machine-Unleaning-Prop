{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9584dc74",
   "metadata": {},
   "source": [
    "# Machine Unlearning + Noise Generator\n",
    "\n",
    "This is a copy of the original `Machine Unlearning.ipynb` notebook, with the key difference of using a different way of generating the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e828435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tt\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "\n",
    "train_new_one = False\n",
    "# torch.manual_seed(100)\n",
    "# After I optimize the Hyperparameters, I want to calculate at least 30 models, to chech the average performance\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a73d496",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e04a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "def training_step(model, batch):\n",
    "    images, labels = batch\n",
    "    images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "    out = model(images)                  \n",
    "    loss = F.cross_entropy(out, labels) \n",
    "    return loss\n",
    "\n",
    "def validation_step(model, batch):\n",
    "    images, labels = batch\n",
    "    images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "    out = model(images)                    \n",
    "    loss = F.cross_entropy(out, labels)   \n",
    "    acc = accuracy(out, labels)\n",
    "    return {'Loss': loss.detach(), 'Acc': acc}\n",
    "\n",
    "def validation_epoch_end(model, outputs):\n",
    "    batch_losses = [x['Loss'] for x in outputs]\n",
    "    epoch_loss = torch.stack(batch_losses).mean()   \n",
    "    batch_accs = [x['Acc'] for x in outputs]\n",
    "    epoch_acc = torch.stack(batch_accs).mean()      \n",
    "    return {'Loss': epoch_loss.item(), 'Acc': epoch_acc.item()}\n",
    "\n",
    "def epoch_end(model, epoch, result):\n",
    "    print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "        epoch, result['lrs'][-1], result['train_loss'], result['Loss'], result['Acc']))\n",
    "    \n",
    "def distance(model,model0):\n",
    "    distance=0\n",
    "    normalization=0\n",
    "    for (k, p), (k0, p0) in zip(model.named_parameters(), model0.named_parameters()):\n",
    "        space='  ' if 'bias' in k else ''\n",
    "        current_dist=(p.data0-p0.data0).pow(2).sum().item()\n",
    "        current_norm=p.data0.pow(2).sum().item()\n",
    "        distance+=current_dist\n",
    "        normalization+=current_norm\n",
    "    print(f'Distance: {np.sqrt(distance)}')\n",
    "    print(f'Normalized Distance: {1.0*np.sqrt(distance/normalization)}')\n",
    "    return 1.0*np.sqrt(distance/normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fec89a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [validation_step(model, batch) for batch in val_loader]\n",
    "    return validation_epoch_end(model, outputs)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "\n",
    "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "    \n",
    "    for epoch in range(epochs): \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = training_step(model, batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            lrs.append(get_lr(optimizer))\n",
    "            \n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        epoch_end(model, epoch, result)\n",
    "        history.append(result)\n",
    "        sched.step(result['Loss'])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c6d890",
   "metadata": {},
   "source": [
    "## Train/Load the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32155eed",
   "metadata": {},
   "source": [
    "### load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b41e0a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./cifar10.tgz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3716/3544667939.py:8: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path='./data')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'train']\n",
      "['bird', 'deer', 'horse', 'automobile', 'frog', 'airplane', 'truck', 'cat', 'dog', 'ship']\n"
     ]
    }
   ],
   "source": [
    "# Dowload the dataset\n",
    "if os.path.exists(\"data/cifar10\"):\n",
    "    dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\"\n",
    "    download_url(dataset_url, '.')\n",
    "\n",
    "    # Extract from archive\n",
    "    with tarfile.open('./cifar10.tgz', 'r:gz') as tar:\n",
    "        tar.extractall(path='./data')\n",
    "        \n",
    "    # Look into the data directory\n",
    "    data_dir = './data/cifar10'\n",
    "    print(os.listdir(data_dir))\n",
    "    classes = os.listdir(data_dir + \"/train\")\n",
    "    print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29db69d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = tt.Compose([\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = tt.Compose([\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27a417a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImageFolder(data_dir+'/train', transform_train)\n",
    "valid_ds = ImageFolder(data_dir+'/test', transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7844cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size*2, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796f4d2b",
   "metadata": {},
   "source": [
    "### Train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54996a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = resnet18(num_classes = 10).to(DEVICE)\n",
    "\n",
    "epochs = 40\n",
    "max_lr = 0.01\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-4\n",
    "opt_func = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6352284",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'os' has no attribute 'exists'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'os' has no attribute 'exists'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if os.exists(\"ResNET18_CIFAR10_ALL_CLASSES.pt\"):\n",
    "    history = fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, \n",
    "                                grad_clip=grad_clip, \n",
    "                                weight_decay=weight_decay, \n",
    "                                opt_func=opt_func)\n",
    "\n",
    "    torch.save(model.state_dict(), \"ResNET18_CIFAR10_ALL_CLASSES.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980397d",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3769eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_new_one:\n",
    "    model.load_state_dict(torch.load(\"ResNET18_CIFAR10_ALL_CLASSES.pt\", map_location=DEVICE))\n",
    "    history = [evaluate(model, valid_dl)]\n",
    "    history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e31ccb",
   "metadata": {},
   "source": [
    "## Unlearning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4696560",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d212fd",
   "metadata": {},
   "source": [
    "Originally used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96f88a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # defining the noise structure\n",
    "# class Noise(nn.Module):\n",
    "#     def __init__(self, *dim):\n",
    "#         super().__init__()\n",
    "#         self.noise = torch.nn.Parameter(torch.randn(*dim), requires_grad = True)\n",
    "        \n",
    "#     def forward(self):\n",
    "#         return self.noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c939dd9",
   "metadata": {},
   "source": [
    "Trying a different approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "287dac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network module for generating noise patterns\n",
    "    through a series of fully connected layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            dim_out: list,\n",
    "            dim_hidden: list = [1000],\n",
    "            dim_start: int = 100,\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Initialize the NoiseGenerator.\n",
    "\n",
    "        Parameters:\n",
    "            dim_out (list): The output dimensions for the generated noise.\n",
    "            dim_hidden (list): The dimensions of hidden layers, defaults to [1000].\n",
    "            dim_start (int): The initial dimension of random noise, defaults to 100.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = dim_out\n",
    "        self.start_dims = dim_start  # Initial dimension of random noise\n",
    "\n",
    "        # Define fully connected layers\n",
    "        self.layers = {}\n",
    "        self.layers[\"l1\"] = nn.Linear(self.start_dims, dim_hidden[0])\n",
    "        last = dim_hidden[0]\n",
    "        for idx in range(len(dim_hidden)-1):\n",
    "            self.layers[f\"l{idx+2}\"] = nn.Linear(dim_hidden[idx], dim_hidden[idx+1])\n",
    "            last = dim_hidden[idx+1]\n",
    "\n",
    "        # Define output layer\n",
    "        self.f_out = nn.Linear(last, math.prod(self.dim))        \n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward pass to transform random noise into structured output.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The reshaped tensor with specified output dimensions.\n",
    "        \"\"\"\n",
    "        # Generate random starting noise\n",
    "        x = torch.randn(self.start_dims)\n",
    "        x = x.flatten()\n",
    "\n",
    "        # Transform noise into learnable patterns\n",
    "        for layer in self.layers.keys():\n",
    "            x = self.layers[layer](x)\n",
    "            x = torch.relu(x)\n",
    "\n",
    "        # Apply output layer\n",
    "        x = self.f_out(x)\n",
    "\n",
    "        # Reshape tensor to the specified dimensions\n",
    "        reshaped_tensor = x.view(self.dim)\n",
    "        return reshaped_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff85afe",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65082f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all classes\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# classes which are required to un-learn\n",
    "classes_to_forget = [0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfedd156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classwise list of samples\n",
    "num_classes = 10\n",
    "classwise_train = {}\n",
    "for i in range(num_classes):\n",
    "    classwise_train[i] = []\n",
    "\n",
    "for img, label in train_ds:\n",
    "    classwise_train[label].append((img, label))\n",
    "    \n",
    "classwise_test = {}\n",
    "for i in range(num_classes):\n",
    "    classwise_test[i] = []\n",
    "\n",
    "for img, label in valid_ds:\n",
    "    classwise_test[label].append((img, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edbda37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting some samples from retain classes\n",
    "num_samples_per_class = 1000\n",
    "\n",
    "retain_samples = []\n",
    "for i in range(len(classes)):\n",
    "    if classes[i] not in classes_to_forget:\n",
    "        retain_samples += classwise_train[i][:num_samples_per_class]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70736605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain validation set\n",
    "retain_valid = []\n",
    "for cls in range(num_classes):\n",
    "    if cls not in classes_to_forget:\n",
    "        for img, label in classwise_test[cls]:\n",
    "            retain_valid.append((img, label))\n",
    "            \n",
    "# forget validation set\n",
    "forget_valid = []\n",
    "for cls in range(num_classes):\n",
    "    if cls in classes_to_forget:\n",
    "        for img, label in classwise_test[cls]:\n",
    "            forget_valid.append((img, label))\n",
    "            \n",
    "forget_valid_dl = DataLoader(forget_valid, batch_size, num_workers=3, pin_memory=True)\n",
    "retain_valid_dl = DataLoader(retain_valid, batch_size*2, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb9afbe",
   "metadata": {},
   "source": [
    "### Training the Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fcc11a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3716/1124881384.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"ResNET18_CIFAR10_ALL_CLASSES.pt\", map_location=DEVICE))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the model\n",
    "model = resnet18(num_classes = 10).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"ResNET18_CIFAR10_ALL_CLASSES.pt\", map_location=DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1170217b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 μs, sys: 0 ns, total: 3 μs\n",
      "Wall time: 4.53 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if train_new_one:\n",
    "    noises = {}\n",
    "    for cls in classes_to_forget:\n",
    "        print(\"Optiming loss for class {}\".format(cls))\n",
    "        noises[cls] = Noise(batch_size, 3, 32, 32)\n",
    "        opt = torch.optim.Adam(noises[cls].parameters(), lr = 0.1)\n",
    "\n",
    "        num_epochs = 5\n",
    "        num_steps = 8\n",
    "        class_label = cls\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = []\n",
    "            for batch in range(num_steps):\n",
    "                inputs = noises[cls]()\n",
    "                labels = torch.zeros(batch_size)+class_label\n",
    "                outputs = model(inputs)\n",
    "                loss = -F.cross_entropy(outputs, labels.long()) + 0.1*torch.mean(torch.sum(torch.square(inputs), [1, 2, 3]))\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                total_loss.append(loss.cpu().detach().numpy())\n",
    "            print(\"Loss: {}\".format(np.mean(total_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a08aa35",
   "metadata": {},
   "source": [
    "## Impair Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09feaed0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'noises' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:8\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'noises' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 256\n",
    "noisy_data = []\n",
    "num_batches = 20\n",
    "class_num = 0\n",
    "\n",
    "for cls in classes_to_forget:\n",
    "    for i in range(num_batches):\n",
    "        batch = noises[cls]().cpu().detach()\n",
    "        for i in range(batch[0].size(0)):\n",
    "            noisy_data.append((batch[i], torch.tensor(class_num)))\n",
    "\n",
    "other_samples = []\n",
    "for i in range(len(retain_samples)):\n",
    "    other_samples.append((retain_samples[i][0].cpu(), torch.tensor(retain_samples[i][1])))\n",
    "noisy_data += other_samples\n",
    "noisy_loader = torch.utils.data.DataLoader(noisy_data, batch_size=256, shuffle = True)\n",
    "\n",
    "\n",
    "if train_new_one:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.02)\n",
    "\n",
    "    for epoch in range(1):  \n",
    "        model.train(True)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0\n",
    "        for i, data in enumerate(noisy_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs,torch.tensor(labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            out = torch.argmax(outputs.detach(),dim=1)\n",
    "            assert out.shape==labels.shape\n",
    "            running_acc += (labels==out).sum().item()\n",
    "        print(f\"Train loss {epoch+1}: {running_loss/len(train_ds)},Train Acc:{running_acc*100/len(train_ds)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac4a772",
   "metadata": {},
   "source": [
    "### Performance after Impair Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfcffec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_new_one:\n",
    "    print(\"Performance of Standard Forget Model on Forget Class\")\n",
    "    history = [evaluate(model, forget_valid_dl)]\n",
    "    print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
    "    print(\"Loss: {}\".format(history[0][\"Loss\"]))\n",
    "\n",
    "    print(\"Performance of Standard Forget Model on Retain Class\")\n",
    "    history = [evaluate(model, retain_valid_dl)]\n",
    "    print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
    "    print(\"Loss: {}\".format(history[0][\"Loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabdfc92",
   "metadata": {},
   "source": [
    "## Repair Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca2abac7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'other_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'other_samples' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "heal_loader = torch.utils.data.DataLoader(other_samples, batch_size=256, shuffle = True)\n",
    "if train_new_one:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "    for epoch in range(1):  \n",
    "        model.train(True)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0\n",
    "        for i, data in enumerate(heal_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs,torch.tensor(labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            out = torch.argmax(outputs.detach(),dim=1)\n",
    "            assert out.shape==labels.shape\n",
    "            running_acc += (labels==out).sum().item()\n",
    "        print(f\"Train loss {epoch+1}: {running_loss/len(train_ds)},Train Acc:{running_acc*100/len(train_ds)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee6e55",
   "metadata": {},
   "source": [
    "### Performance after Repair Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e74aa345",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_new_one:\n",
    "    print(\"Performance of Standard Forget Model on Forget Class\")\n",
    "    history = [evaluate(model, forget_valid_dl)]\n",
    "    print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
    "    print(\"Loss: {}\".format(history[0][\"Loss\"]))\n",
    "\n",
    "    print(\"Performance of Standard Forget Model on Retain Class\")\n",
    "    history = [evaluate(model, retain_valid_dl)]\n",
    "    print(\"Accuracy: {}\".format(history[0][\"Acc\"]*100))\n",
    "    print(\"Loss: {}\".format(history[0][\"Loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c19a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def load_models_dict(path: str=\"data/new/models\") -> Dict[str, torch.nn.Module]:\n",
    "    de = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = resnet18(num_classes = 10).to(de)\n",
    "    \n",
    "    # load all the models\n",
    "    md = {}\n",
    "    for list in os.listdir(path):\n",
    "        \n",
    "        model.load_state_dict(torch.load(f=os.path.join(path, list), map_location=DEVICE, weights_only=True))\n",
    "        model.eval()\n",
    "        md[len(md)] = model\n",
    "\n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2281224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.fyemu_tunable import main\n",
    "\n",
    "for i in range(0):\n",
    "    model = main(\n",
    "        new_baseline=False,\n",
    "        logs=False,\n",
    "        model_eval_logs=False,\n",
    "    )\n",
    "    \n",
    "    if not os.path.exists(\"data/new/models\"):\n",
    "        os.makedirs(\"data/new/models\")\n",
    "    n = len(os.listdir(\"data/new/models\"))\n",
    "    torch.save(model.state_dict(), f\"data/new/models/ResNET18_CIFAR10_UN_{n}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f11c592",
   "metadata": {},
   "source": [
    "___\n",
    "## Evaluate multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bee59dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import tarfile\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as tt\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Firstly, all the data\n",
    "class SubData(Dataset):\n",
    "    def __init__(self, data, transform):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.data[idx]\n",
    "\n",
    "        img = Image.open(f\"{path}\").convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        label = torch.tensor(label)\n",
    "        return img.to(self.device), label.to(self.device)\n",
    "\n",
    "data_dir = f'data{os.sep}cifar10'\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# classes which are required to un-learn\n",
    "classes_to_forget = [0, 2]\n",
    "\n",
    "transform_test = tt.Compose([\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_train = tt.Compose([\n",
    "    tt.ToTensor(),\n",
    "    tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb5e2acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_all_ds = ImageFolder(data_dir+f'{os.sep}test', transform_test)\n",
    "valid_all_dl = DataLoader(valid_all_ds, 256,)\n",
    "\n",
    "train_all_ds = ImageFolder(data_dir+f'{os.sep}train', transform_train)\n",
    "train_all_dl = DataLoader(train_all_ds, 256,)\n",
    "\n",
    "rt_tr = {}\n",
    "for t, l in train_all_ds.imgs:\n",
    "    if l not in classes_to_forget:\n",
    "        rt_tr[len(rt_tr)] = (t, l)\n",
    "rt_vl = {}\n",
    "for t, l in valid_all_ds.imgs:\n",
    "    if l not in classes_to_forget:\n",
    "        rt_vl[len(rt_vl)] = (t, l)\n",
    "\n",
    "train_retain_ds = SubData(rt_tr, transform_train)\n",
    "valid_retain_ds = SubData(rt_vl, transform_test)\n",
    "\n",
    "train_retain_dl = DataLoader(train_retain_ds, 256, shuffle=True)\n",
    "valid_retain_dl = DataLoader(valid_retain_ds, 256*2)\n",
    "\n",
    "rt_tr = {}\n",
    "for t, l in train_all_ds.imgs:\n",
    "    if l in classes_to_forget:\n",
    "        rt_tr[len(rt_tr)] = (t, l)\n",
    "rt_vl = {}\n",
    "for t, l in valid_all_ds.imgs:\n",
    "    if l in classes_to_forget:\n",
    "        rt_vl[len(rt_vl)] = (t, l)\n",
    "\n",
    "train_forget_ds = SubData(rt_tr, transform_train)\n",
    "valid_forget_ds = SubData(rt_vl, transform_test)\n",
    "\n",
    "train_forget_dl = DataLoader(train_forget_ds, 256, shuffle=True)\n",
    "valid_forget_dl = DataLoader(train_forget_ds, 256*2)\n",
    "\n",
    "loaders = {\n",
    "    0: train_all_dl,\n",
    "    1: valid_all_dl,\n",
    "    2: train_retain_dl,\n",
    "    3: valid_retain_dl,\n",
    "    4: train_forget_dl,\n",
    "    5: valid_forget_dl,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5c37661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.metrics\n",
    "from src.fyemu_tunable import evaluate\n",
    "paper_ms    = load_models_dict(path=\"data/paper/models\")\n",
    "gemu_ms     = load_models_dict(path=\"data/new/models\")\n",
    "\n",
    "exact_ms    = load_models_dict(path=\"data/retrain/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdd61db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'Loss': 2.125023365020752, 'Acc': 0.8007015585899353},\n",
       " 1: {'Loss': 3.0320842266082764, 'Acc': 0.643359363079071},\n",
       " 2: {'Loss': 0.0003286290739197284, 'Acc': 1.0},\n",
       " 3: {'Loss': 1.1749095916748047, 'Acc': 0.7978271245956421},\n",
       " 4: {'Loss': 10.641883850097656, 'Acc': 0.0},\n",
       " 5: {'Loss': 10.659263610839844, 'Acc': 0.0}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run(model, loaders):\n",
    "    results = {}\n",
    "    for name, loader in loaders.items():\n",
    "        results[name] = evaluate(model.to(DEVICE), loader)\n",
    "\n",
    "    return results\n",
    "run(exact_ms[0], loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5922b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "kl_exact_to_fyemu = {key: src.metrics.kl_divergence_between_models(exact_ms[0], paper_ms[0], loader) for key, loader in loaders.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea4b7584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 3.001258940416939,\n",
       " 1: 2.3969006985425954,\n",
       " 2: 2.9579621895103707,\n",
       " 3: 2.253737926483154,\n",
       " 4: 3.1858519375324255,\n",
       " 5: 3.2036361098289494}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_exact_to_fyemu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ec23f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445,\n",
       "  2.99800184247445],\n",
       " 1: [2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745,\n",
       "  2.4138207420706745],\n",
       " 2: [2.9805936950027556,\n",
       "  2.985043885601553,\n",
       "  2.981307994028564,\n",
       "  2.984555946034232,\n",
       "  2.979505976294256,\n",
       "  2.98457421163085,\n",
       "  2.9801700039274386,\n",
       "  2.9795582825970484,\n",
       "  2.984668875955472,\n",
       "  2.9838554509885764,\n",
       "  2.9817508557799512,\n",
       "  2.9816223663888914,\n",
       "  2.982272570300253,\n",
       "  2.983155431261489,\n",
       "  2.9830585953536293,\n",
       "  2.982755161394738,\n",
       "  2.9785177206537523,\n",
       "  2.9855474651239486,\n",
       "  2.9839427121885262,\n",
       "  2.982957079152394,\n",
       "  2.9789411338271607,\n",
       "  2.983763217926025,\n",
       "  2.9828373079846617,\n",
       "  2.9823159867790854,\n",
       "  2.9805902872875243,\n",
       "  2.9814254539028107,\n",
       "  2.9798289150189436,\n",
       "  2.980944419362743,\n",
       "  2.980003653058581,\n",
       "  2.9793809280274033],\n",
       " 3: [2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787,\n",
       "  2.3201623298227787],\n",
       " 4: [3.0582285404205325,\n",
       "  3.05997970700264,\n",
       "  3.0537044346332554,\n",
       "  3.0470104694366458,\n",
       "  3.0574247241020203,\n",
       "  3.059084576368332,\n",
       "  3.078172659873962,\n",
       "  3.06516552567482,\n",
       "  3.0929919421672825,\n",
       "  3.0890380203723904,\n",
       "  3.048155945539474,\n",
       "  3.0667095899581907,\n",
       "  3.0586705744266514,\n",
       "  3.050057291984558,\n",
       "  3.05141339302063,\n",
       "  3.073940873146057,\n",
       "  3.0595492601394656,\n",
       "  3.090350687503815,\n",
       "  3.0501386821269985,\n",
       "  3.0550476908683777,\n",
       "  3.0654064357280726,\n",
       "  3.0501775920391085,\n",
       "  3.054583549499512,\n",
       "  3.069997739791871,\n",
       "  3.0879609227180493,\n",
       "  3.063188612461091,\n",
       "  3.0801770269870756,\n",
       "  3.0655084013938896,\n",
       "  3.054896235466004,\n",
       "  3.085013377666473],\n",
       " 5: [3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569,\n",
       "  3.072323250770569]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_exact_to_gemu = {}\n",
    "for model in gemu_ms.values():\n",
    "    new = {key: src.metrics.kl_divergence_between_models(exact_ms[0], model, loader) for key, loader in loaders.items()}\n",
    "    if len(kl_exact_to_gemu) == 0:\n",
    "        for name in new.keys():\n",
    "            kl_exact_to_gemu[name] = []\n",
    "    for name in new.keys():\n",
    "        kl_exact_to_gemu[name].append(new[name])\n",
    "kl_exact_to_gemu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc403256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2.99800184247445,\n",
       " 1: 2.4138207420706745,\n",
       " 2: 2.9819815194277752,\n",
       " 3: 2.3201623298227787,\n",
       " 4: 3.0647248160839085,\n",
       " 5: 3.072323250770569}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key in kl_exact_to_gemu.keys():\n",
    "    kl_exact_to_gemu[key] = sum(kl_exact_to_gemu[key])/len(kl_exact_to_gemu[key])\n",
    "kl_exact_to_gemu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "095208fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Literal\n",
    "\n",
    "def create_boxplots(score_lists1: Dict[str, List[float]], score_lists2: Dict[str, List[float]], score_lists3: Dict[str, List[float]], title: str = 'Box Plot of Accuracy Scores for Different Models', evaluation: Literal[\"Accuracy\", \"Loss\"] = \"Accuracy\") -> None:\n",
    "    \"\"\"Create a box plot of accuracy scores for each parsed list in the diconary.\"\"\"\n",
    "\n",
    "    # Prepare data for the box plot\n",
    "    data1 = [sum(scores)/len(scores) for scores in score_lists1.values()]\n",
    "    data2 = [sum(scores)/len(scores) for scores in score_lists2.values()]\n",
    "    data3 = [sum(scores)/len(scores) for scores in score_lists3.values()]\n",
    "    labels = list(score_lists1.keys())\n",
    "    X_axis = (np.arange(len(labels)))\n",
    "\n",
    "    # Create the box plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x=X_axis-0.3   , height=data1, width=0.3, color=\"blue\", label=\"Baseline\")\n",
    "    plt.bar(x=X_axis        , height=data2, width=0.3, color=\"red\", label=\"FEMU\")\n",
    "    plt.bar(x=X_axis+0.3   , height=data3, width=0.3, color=\"orange\", label=\"FEMU+Gen\")\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xticks(X_axis, labels)\n",
    "    plt.xlabel('Subsets')\n",
    "    plt.ylabel(f'{evaluation} Score')\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c7b932",
   "metadata": {},
   "source": [
    "### Accuracy Per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47c7551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import tarfile\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as tt\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "data_classes = {}\n",
    "for c in classes:\n",
    "    data_classes[c] = {}\n",
    "    for t, l in valid_all_ds.imgs:\n",
    "        if l == c:\n",
    "            data_classes[c][len(data_classes[c])] = (t, l)\n",
    "\n",
    "data_dl_classes = {c: DataLoader(SubData(data_classes[c], transform_test), 256) for c in classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4887f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_accs_paper = {c: [] for c in classes}\n",
    "for idx, model in paper_ms.items():\n",
    "    model.eval()\n",
    "    accs = run(model, data_dl_classes)\n",
    "    for c, acc in accs.items():\n",
    "        class_accs_paper[c].append(acc[\"Acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b73c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_accs_gemu = {c: [] for c in classes}\n",
    "for idx, model in gemu_ms.items():\n",
    "    model.eval()\n",
    "    accs = run(model, data_dl_classes)\n",
    "    for c, acc in accs.items():\n",
    "        class_accs_gemu[c].append(acc[\"Acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b134631",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_accs_exact = {c: [] for c in classes}\n",
    "for idx, model in exact_ms.items():\n",
    "    model.eval()\n",
    "    accs = run(model, data_dl_classes)\n",
    "    for c, acc in accs.items():\n",
    "        class_accs_exact[c].append(acc[\"Acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83c8a7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAIjCAYAAAA0vUuxAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAVZZJREFUeJzt3Xt8z/X///H7e+ej42YbxpznkNMcUiE1Ic2hiFHOKk0OU59ITpFFJUURhQ5knUgqYjl0IKdGcoowZEPYGG1sr98f/by/3m3z8mbzfq/drpfL61Lv5+v5er0er9fes933fL2eb4thGIYAAAAAAHlycXQBAAAAAODsCE4AAAAAYILgBAAAAAAmCE4AAAAAYILgBAAAAAAmCE4AAAAAYILgBAAAAAAmCE4AAAAAYILgBAAAAAAmCE4AANyAsLAw9enTx9Fl3LQVK1aofv368vLyksVi0dmzZx1dUg4Wi0Xjx4+3e7tDhw7JYrFowYIF+V4TgKKH4ASgyHrrrbdksVjUtGlTR5dSKKWkpOjpp59WeHi4fHx85Ovrq4iICE2aNMkpf/lGTn/99ZcefvhheXt7680339QHH3wgX1/fXPsuWLBAFotFFotFP/zwQ471hmEoNDRUFotFDzzwQEGXDgC3nJujCwAAR1m4cKHCwsK0adMm7d+/X1WrVnV0SYXG5s2bdf/99+v8+fN65JFHFBERIUnasmWLXnrpJa1fv17ffvutg6ssWHv37pWLS+H+++PmzZt17tw5TZw4UZGRkde1jZeXlxYtWqS77rrLpn3dunU6evSoPD09C6JUAHC4wv0vPgDcoIMHD+qnn37StGnTFBgYqIULFzq6pDylp6c7ugQbZ8+eVefOneXq6qpffvlFc+fO1RNPPKEnnnhC77zzjg4cOKAWLVo4uswCYRiGLl68KEny9PSUu7u7gyu6OSdOnJAklShR4rq3uf/++/XJJ5/o8uXLNu2LFi1SRESEgoOD87NEAHAaBCcARdLChQtVsmRJtW/fXl26dMkzOJ09e1bDhw9XWFiYPD09Vb58efXq1UunTp2y9vn77781fvx4Va9eXV5eXgoJCdGDDz6oAwcOSJLWrl0ri8WitWvX2uw7t+cv+vTpIz8/Px04cED333+//P391bNnT0nS999/r65du6pChQry9PRUaGiohg8fbv1F/mp79uzRww8/rMDAQHl7e6tGjRoaPXq0JGnNmjWyWCxasmRJju0WLVoki8WiDRs25Hnt3n77bR07dkzTpk1TeHh4jvVBQUF6/vnnbdreeust1a5dW56enipbtqxiYmJy3M539913q06dOtqxY4datmwpHx8fVa1aVZ9++qmkf0Y0mjZtaj2f1atX22w/fvx4WSwW67kXK1ZMpUuX1tChQ/X333/b9J0/f77uuecelSlTRp6enqpVq5ZmzZqV41zCwsL0wAMPaOXKlWrUqJG8vb319ttvW9dd/YzTpUuXNGHCBFWrVk1eXl4qXbq07rrrLq1atcpmn999952aN28uX19flShRQh07dtTu3btzPZf9+/erT58+KlGihIoXL66+ffvqwoULuXxVcvrkk08UEREhb29vBQQE6JFHHtGxY8dsrnfv3r0lSY0bN5bFYrmuZ7aio6P1119/2ZxXZmamPv30U/Xo0SPXbdLT0zVixAiFhobK09NTNWrU0CuvvCLDMGz6ZWRkaPjw4QoMDJS/v786dOigo0eP5rrPY8eOqV+/fgoKCpKnp6dq166tefPmmdafnJysvn37qnz58vL09FRISIg6duyoQ4cOmW4LoGjjVj0ARdLChQv14IMPysPDQ9HR0Zo1a5Y2b96sxo0bW/ucP39ezZs31+7du9WvXz81bNhQp06d0rJly3T06FEFBAQoKytLDzzwgBISEtS9e3cNHTpU586d06pVq7Rz505VqVLF7touX76sNm3a6K677tIrr7wiHx8fSf/8InzhwgUNGjRIpUuX1qZNmzRjxgwdPXpUn3zyiXX7HTt2qHnz5nJ3d9djjz2msLAwHThwQF9++aVefPFF3X333QoNDdXChQvVuXPnHNelSpUqatasWZ71LVu2TN7e3urSpct1nc/48eM1YcIERUZGatCgQdq7d6/1ev/44482ozZnzpzRAw88oO7du6tr166aNWuWunfvroULF2rYsGF64okn1KNHD7388svq0qWLjhw5In9/f5vjPfzwwwoLC1NcXJw2btyoN954Q2fOnNH7779v7TNr1izVrl1bHTp0kJubm7788ks9+eSTys7OVkxMjM3+9u7dq+joaD3++OMaOHCgatSoked5xsXFacCAAWrSpInS0tK0ZcsWbdu2Ta1bt5YkrV69Wu3atVPlypU1fvx4Xbx4UTNmzNCdd96pbdu2KSwsLMe5VKpUSXFxcdq2bZveeecdlSlTRlOmTLnmNV+wYIH69u2rxo0bKy4uTikpKXr99df1448/6pdfflGJEiU0evRo1ahRQ3PmzNELL7ygSpUqXdf7NSwsTM2aNdNHH32kdu3aSZK++eYbpaamqnv37nrjjTds+huGoQ4dOmjNmjXq37+/6tevr5UrV+qZZ57RsWPH9Nprr1n7DhgwQB9++KF69OihO+64Q999953at2+fo4aUlBTdfvvtslgsGjx4sAIDA/XNN9+of//+SktL07Bhw/Ks/6GHHtJvv/2mp556SmFhYTpx4oRWrVqlpKSkHNcfAGwYAFDEbNmyxZBkrFq1yjAMw8jOzjbKly9vDB061Kbf2LFjDUnG559/nmMf2dnZhmEYxrx58wxJxrRp0/Lss2bNGkOSsWbNGpv1Bw8eNCQZ8+fPt7b17t3bkGSMHDkyx/4uXLiQoy0uLs6wWCzG4cOHrW0tWrQw/P39bdqurscwDGPUqFGGp6encfbsWWvbiRMnDDc3N2PcuHE5jnO1kiVLGvXq1btmn6v36eHhYdx3331GVlaWtX3mzJmGJGPevHnWtpYtWxqSjEWLFlnb9uzZY0gyXFxcjI0bN1rbV65cmePajRs3zpBkdOjQwaaGJ5980pBkbN++3dqW27Vs06aNUblyZZu2ihUrGpKMFStW5OhfsWJFo3fv3tbX9erVM9q3b3+Nq2EY9evXN8qUKWP89ddf1rbt27cbLi4uRq9evXKcS79+/Wy279y5s1G6dOlrHiMzM9MoU6aMUadOHePixYvW9uXLlxuSjLFjx1rb5s+fb0gyNm/efM19/rvvzJkzDX9/f+t17Nq1q9GqVSvDMP65Lldfh6VLlxqSjEmTJtnsr0uXLobFYjH2799vGIZhJCYmGpKMJ5980qZfjx49DEk278v+/fsbISEhxqlTp2z6du/e3ShevLi1rn9/j505c8aQZLz88sum5wsA/8ategCKnIULFyooKEitWrWS9M9Ux926ddPixYuVlZVl7ffZZ5+pXr16OUZlrmxzpU9AQICeeuqpPPvciEGDBuVo8/b2tv5/enq6Tp06pTvuuEOGYeiXX36RJJ08eVLr169Xv379VKFChTzr6dWrlzIyMqy3wUlSfHy8Ll++rEceeeSataWlpeUY5cnL6tWrlZmZqWHDhtlMpDBw4EAVK1ZMX331lU1/Pz8/de/e3fq6Ro0aKlGihGrWrGkz++GV///jjz9yHPPfI0ZXvjZff/21te3qa5mamqpTp06pZcuW+uOPP5SammqzfaVKldSmTRvTcy1RooR+++03/f7777muP378uBITE9WnTx+VKlXK2l63bl21bt3apr4rnnjiCZvXzZs3119//aW0tLQ869iyZYtOnDihJ598Ul5eXtb29u3bKzw8PMc1vxEPP/ywLl68qOXLl+vcuXNavnx5nrfpff3113J1ddWQIUNs2keMGCHDMPTNN99Y+0nK0e/fo0eGYeizzz5TVFSUDMPQqVOnrEubNm2Umpqqbdu25VqLt7e3PDw8tHbtWp05c+ZGTh1AEUZwAlCkZGVlafHixWrVqpUOHjyo/fv3a//+/WratKlSUlKUkJBg7XvgwAHVqVPnmvs7cOCAatSoITe3/Lvz2c3NTeXLl8/RnpSUZP2l28/PT4GBgWrZsqUkWX/ZvxIkzOoODw9X48aNbZ7tWrhwoW6//XbT2QWLFSumc+fOXde5HD58WJJy3N7m4eGhypUrW9dfUb58+RyBs3jx4goNDc3RJinXX36rVatm87pKlSpycXGxeYblxx9/VGRkpPU5o8DAQD333HOSlGtwuh4vvPCCzp49q+rVq+u2227TM888ox07dljX53UtJKlmzZo6depUjolA/h1+S5YsKSn3876e44SHh+e45jciMDBQkZGRWrRokT7//HNlZWXleevm4cOHVbZs2Rxhu2bNmjb1Hj58WC4uLjluF/z3eZw8eVJnz57VnDlzFBgYaLP07dtX0v9NevFvnp6emjJlir755hsFBQWpRYsWmjp1qpKTk+2/CACKHJ5xAlCkfPfddzp+/LgWL16sxYsX51i/cOFC3Xffffl6zLxGnq4e3bqap6dnjmmus7Ky1Lp1a50+fVrPPvuswsPD5evrq2PHjqlPnz7Kzs62u65evXpp6NChOnr0qDIyMrRx40bNnDnTdLvw8HAlJiYqMzNTHh4edh/3WlxdXe1qN/41uUBu/n39Dxw4oHvvvVfh4eGaNm2aQkND5eHhoa+//lqvvfZajmt59ejUtbRo0UIHDhzQF198oW+//VbvvPOOXnvtNc2ePVsDBgy4rn38282cd0Hr0aOHBg4cqOTkZLVr186umfluxpWvzyOPPGKd3OLf6tatm+f2w4YNU1RUlJYuXaqVK1dqzJgxiouL03fffacGDRoUSM0A/hsITgCKlIULF6pMmTJ68803c6z7/PPPtWTJEs2ePVve3t6qUqWKdu7cec39ValSRT///LMuXbqU59TUV0YJ/j2LnD1/+f/111+1b98+vffee+rVq5e1/d8ztlWuXFmSTOuWpO7duys2NlYfffSRLl68KHd3d3Xr1s10u6ioKG3YsEGfffaZoqOjr9m3YsWKkv6ZYOFKbdI/s7AdPHjwuj87yB6///67zSjR/v37lZ2dbX3w/8svv1RGRoaWLVtmM6KzZs2amz52qVKl1LdvX/Xt21fnz59XixYtNH78eA0YMMDmWvzbnj17FBAQkOeHz9rj6uPcc889Nuv27t1rXX+zOnfurMcff1wbN25UfHz8NetZvXq1zp07ZzPqtGfPHpt6K1asqOzsbOso7tU1X+3KjHtZWVk3/P6pUqWKRowYoREjRuj3339X/fr19eqrr+rDDz+8of0BKBq4VQ9AkXHx4kV9/vnneuCBB9SlS5ccy+DBg3Xu3DktW7ZM0j+zb23fvj3Xabuv/MX/oYce0qlTp3IdqbnSp2LFinJ1ddX69ett1r/11lvXXfuVkYerRxoMw9Drr79u0y8wMFAtWrTQvHnzlJSUlGs9VwQEBKhdu3b68MMPtXDhQrVt21YBAQGmtTzxxBMKCQnRiBEjtG/fvhzrT5w4oUmTJkmSIiMj5eHhoTfeeMPm+O+++65SU1NznTHtZv07FM+YMUOSrDPA5XYtU1NTNX/+/Js67l9//WXz2s/PT1WrVlVGRoYkKSQkRPXr19d7771nE6J37typb7/9Vvfff/9NHf+KRo0aqUyZMpo9e7b12NI/M9/t3r073665n5+fZs2apfHjxysqKirPfvfff7+ysrJyfI+89tprslgs1q/Llf/+e1a+6dOn27x2dXXVQw89pM8++yzXPxCcPHkyz1ouXLiQY2r6KlWqyN/f3+ZaAUBuGHECUGQsW7ZM586dU4cOHXJdf/vtt1s/DLdbt2565pln9Omnn6pr167q16+fIiIidPr0aS1btkyzZ89WvXr11KtXL73//vuKjY3Vpk2b1Lx5c6Wnp2v16tV68skn1bFjRxUvXlxdu3bVjBkzZLFYVKVKFS1fvjzP5zByEx4eripVqujpp5/WsWPHVKxYMX322We5Puvyxhtv6K677lLDhg312GOPqVKlSjp06JC++uorJSYm2vTt1auX9dmUiRMnXlctJUuW1JIlS3T//ferfv36euSRRxQRESFJ2rZtmz766CPrdOaBgYEaNWqUJkyYoLZt26pDhw7au3ev3nrrLTVu3Nh0IoobcfDgQXXo0EFt27bVhg0brNNb16tXT5J03333ycPDQ1FRUXr88cd1/vx5zZ07V2XKlNHx48dv+Li1atXS3XffrYiICJUqVUpbtmzRp59+qsGDB1v7vPzyy2rXrp2aNWum/v37W6cjL168uMaPH3+zpy5Jcnd315QpU9S3b1+1bNlS0dHR1unIw8LCNHz48Hw5jqQ8b5W7WlRUlFq1aqXRo0fr0KFDqlevnr799lt98cUXGjZsmPWZpvr16ys6OlpvvfWWUlNTdccddyghIUH79+/Psc+XXnpJa9asUdOmTTVw4EDVqlVLp0+f1rZt27R69WqdPn0611r27dune++9Vw8//LBq1aolNzc3LVmyRCkpKTaTkgBArhwylx8AOEBUVJTh5eVlpKen59mnT58+hru7u3Wa47/++ssYPHiwUa5cOcPDw8MoX7680bt3b5tpkC9cuGCMHj3aqFSpkuHu7m4EBwcbXbp0MQ4cOGDtc/LkSeOhhx4yfHx8jJIlSxqPP/64sXPnzlynI/f19c21tl27dhmRkZGGn5+fERAQYAwcONDYvn17jn0YhmHs3LnT6Ny5s1GiRAnDy8vLqFGjhjFmzJgc+8zIyDBKlixpFC9e3Gbq6uvx559/GsOHDzeqV69ueHl5GT4+PkZERITx4osvGqmpqTZ9Z86caYSHhxvu7u5GUFCQMWjQIOPMmTM2fVq2bGnUrl07x3H+Pb31FZKMmJgY6+srU3jv2rXL6NKli+Hv72+ULFnSGDx4cI5zW7ZsmVG3bl3Dy8vLCAsLM6ZMmWKdWv7gwYOmx76y7urpyCdNmmQ0adLEKFGihOHt7W2Eh4cbL774opGZmWmz3erVq40777zT8Pb2NooVK2ZERUUZu3btsulz5VxOnjxp035lSvCra8xLfHy80aBBA8PT09MoVaqU0bNnT+Po0aO57s/e6civJbdrdu7cOWP48OFG2bJlDXd3d6NatWrGyy+/bDNFvmEYxsWLF40hQ4YYpUuXNnx9fY2oqCjjyJEjOaYjNwzDSElJMWJiYozQ0FDr9929995rzJkzx9rn39ORnzp1yoiJiTHCw8MNX19fo3jx4kbTpk2Njz/+2PT8AcBiGE7whCkAwCEuX76ssmXLKioqSu+++66jy7kpVz5o9+TJk9d1yyEAAPbgGScAKMKWLl2qkydP2kw4AQAAcuIZJwAogn7++Wft2LFDEydOVIMGDayfBwUAAHLHiBMAFEGzZs3SoEGDVKZMGb3//vuOLgcAAKfn0OC0fv16RUVFqWzZsrJYLFq6dKnpNmvXrlXDhg3l6empqlWrasGCBQVeJwD81yxYsECXL1/Wli1bVKdOHUeXky/Gjx8vwzB4vgkAUCAcGpzS09NVr169XD+IMjcHDx5U+/bt1apVKyUmJmrYsGEaMGCAVq5cWcCVAgAAACjKnGZWPYvFoiVLlqhTp0559nn22Wf11Vdf2XzgXffu3XX27FmtWLHiFlQJAAAAoCgqVJNDbNiwQZGRkTZtbdq00bBhw/LcJiMjw+bTwLOzs3X69GmVLl1aFouloEoFAAAA4OQMw9C5c+dUtmxZubhc+2a8QhWckpOTFRQUZNMWFBSktLQ0Xbx4Ud7e3jm2iYuL04QJE25ViQAAAAAKmSNHjqh8+fLX7FOogtONGDVqlGJjY62vU1NTVaFCBR05ckTFihVzYGUAAAAAHCktLU2hoaHy9/c37VuoglNwcLBSUlJs2lJSUlSsWLFcR5skydPTU56enjnaixUrRnACAAAAcF2P8BSqz3Fq1qyZEhISbNpWrVqlZs2aOagiAAAAAEWBQ4PT+fPnlZiYqMTEREn/TDeemJiopKQkSf/cZterVy9r/yeeeEJ//PGH/ve//2nPnj1666239PHHH2v48OGOKB8AAABAEeHQ4LRlyxY1aNBADRo0kCTFxsaqQYMGGjt2rCTp+PHj1hAlSZUqVdJXX32lVatWqV69enr11Vf1zjvvqE2bNg6pHwAAAEDR4DSf43SrpKWlqXjx4kpNTeUZJwCQlJWVpUuXLjm6DNxCrq6ucnNz42M5ABR59mSDQjU5BAAgf50/f15Hjx5VEfsbGiT5+PgoJCREHh4eji4FAAoFghMAFFFZWVk6evSofHx8FBgYyOhDEWEYhjIzM3Xy5EkdPHhQ1apVM/3QRwAAwQkAiqxLly7JMAwFBgbm+ZEO+G/y9vaWu7u7Dh8+rMzMTHl5eTm6JABwevyJCQCKOEaaiiZGmQDAPvyrCQAAAAAmCE4AAAAAYILgBACwYbHc2qUwCgsL0/Tp062vLRaLli5d6rB6AAAFj+AEAChU+vTpI4vFYl1Kly6ttm3baseOHQ6r6fjx42rXrp3Djg8AKHgEJwBAodO2bVsdP35cx48fV0JCgtzc3PTAAw84rJ7g4GB5eno67PgAgIJHcAIAFDqenp4KDg5WcHCw6tevr5EjR+rIkSM6efKkJOnZZ59V9erV5ePjo8qVK2vMmDG6dOmSdfvt27erVatW8vf3V7FixRQREaEtW7ZY1//www9q3ry5vL29FRoaqiFDhig9PT3Peq6+Ve/QoUOyWCz6/PPP1apVK/n4+KhevXrasGGDzTb2HgMA4FgEJwBAoXb+/Hl9+OGHqlq1qkqXLi1J8vf314IFC7Rr1y69/vrrmjt3rl577TXrNj179lT58uW1efNmbd26VSNHjpS7u7sk6cCBA2rbtq0eeugh7dixQ/Hx8frhhx80ePBgu+oaPXq0nn76aSUmJqp69eqKjo7W5cuX8/UYAIBbhw/ABQAUOsuXL5efn58kKT09XSEhIVq+fLn1s4mef/55a9+wsDA9/fTTWrx4sf73v/9JkpKSkvTMM88oPDxcklStWjVr/7i4OPXs2VPDhg2zrnvjjTfUsmVLzZo167o/LPbpp59W+/btJUkTJkxQ7dq1tX//foWHh+fbMQAAtw4jTgCAQqdVq1ZKTExUYmKiNm3apDZt2qhdu3Y6fPiwJCk+Pl533nmngoOD5efnp+eff15JSUnW7WNjYzVgwABFRkbqpZde0oEDB6zrtm/frgULFsjPz8+6tGnTRtnZ2Tp48OB111i3bl3r/4eEhEiSTpw4ka/HAADcOgQnAECh4+vrq6pVq6pq1apq3Lix3nnnHaWnp2vu3LnasGGDevbsqfvvv1/Lly/XL7/8otGjRyszM9O6/fjx4/Xbb7+pffv2+u6771SrVi0tWbJE0j+3/j3++OPWYJaYmKjt27fr999/V5UqVa67xiu3/kn/PAMlSdnZ2fl6DADArcOtegCAQs9iscjFxUUXL17UTz/9pIoVK2r06NHW9VdGoq5WvXp1Va9eXcOHD1d0dLTmz5+vzp07q2HDhtq1a5eqVq1aYPXeimMAAPIXI04AgEInIyNDycnJSk5O1u7du/XUU0/p/PnzioqKUrVq1ZSUlKTFixfrwIEDeuONN6yjSZJ08eJFDR48WGvXrtXhw4f1448/avPmzapZs6akf2bk++mnnzR48GAlJibq999/1xdffJGvEzfcimMAAPIXI04AABuG4egKzK1YscL63JC/v7/Cw8P1ySef6O6775YkDR8+XIMHD1ZGRobat2+vMWPGaPz48ZIkV1dX/fXXX+rVq5dSUlIUEBCgBx98UBMmTJD0z7NJ69at0+jRo9W8eXMZhqEqVaqoW7du+Vb/rTgGACB/WQyjMPyIzD9paWkqXry4UlNTVaxYMUeXAwAO8/fff+vgwYOqVKkSs7gVQXz9AcC+bMCtegAAAABgguAEAAAAACYITgAAAABgguAEAAAAACYITgAAAABgguAEAAAAACYITgAAAABgguAEAAAAACYITgAAAABgws3RBQAAnIzFcmuPZxi39ngAANwARpwAAIVKnz59ZLFYciz79+/Pc13btm2t24eFhclisWjx4sU59l27dm1ZLBYtWLDA2maxWLR06dJc6+jUqVMBnCEAwBkx4gQAKHTatm2r+fPn27QFBgbmuc7T09PmdWhoqObPn6/u3btb2zZu3Kjk5GT5+voWUNUAgMKMEScAQKHj6emp4OBgm8XV1TXPdSVLlrTZvmfPnlq3bp2OHDlibZs3b5569uwpNzf+pggAyIngBAAocoKCgtSmTRu99957kqQLFy4oPj5e/fr1c3BlAABnRXACABQ6y5cvl5+fn3Xp2rVrnuv8/Pw0efLkHPvo16+fFixYIMMw9Omnn6pKlSqqX7/+LTwLAEBhwv0IAIBCp1WrVpo1a5b19dXPJf17nSSVKlUqxz7at2+vxx9/XOvXr9e8efMYbQIAXBPBCQBQ6Pj6+qpq1ap2r7uam5ubHn30UY0bN04///yzlixZkms/f39/paam5mg/e/asihcvbl/hAIBCi1v1AABFVr9+/bRu3Tp17NgxxwQSV9SoUUNbt261acvKytL27dtVvXr1W1EmAMAJMOIEAPhPycjIUHJysk2bm5ubAgICcvStWbOmTp06JR8fnzz3Fxsbq/79+ys8PFytW7dWenq6ZsyYoTNnzmjAgAH5Xj8AwDkRnAAAtgzD0RXclBUrVigkJMSmrUaNGtqzZ0+u/UuXLn3N/UVHR8swDE2bNk0jR46Uj4+PIiIitH79egUFBeVb3QAA52YxjEL+E9JOaWlpKl68uFJTU1WsWDFHlwMADvP333/r4MGDqlSpkry8vBxdDm6x/+zXf5GlYPbbo0j9ugQUGfZkA55xAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMMHnOAEAAKBAWApgdvgC+yCdgpjKnmns/1MYcQIAAAAAEwQnAAAAADDBrXoAAFsFcbvKtXArCwCgEGDECQBQqPTp00cWiyXHsn///jzXtW3b1rp9WFiYLBaLFi9enGPftWvXlsVi0YIFC6xtFotFS5cuzbWOTp063dS5fPbZZ7rnnntUsmRJeXt7q0aNGurXr59++eWXm9ovACD/EZwAAIVO27Ztdfz4cZulUqVKea776KOPbLYPDQ3V/Pnzbdo2btyo5ORk+fr65kuNYWFhWrt2bZ7rn332WXXr1k3169fXsmXLtHfvXi1atEiVK1fWqFGj8qUGAED+ITgBAAodT09PBQcH2yyurq55ritZsqTN9j179tS6det05MgRa9u8efPUs2dPubkV/F3sGzdu1NSpUzVt2jRNmzZNzZs3V4UKFRQREaHnn39e33zzjU3/L774Qg0bNpSXl5cqV66sCRMm6PLly9b1FotF77zzjjp37iwfHx9Vq1ZNy5YtK/DzAICihOAEAChygoKC1KZNG7333nuSpAsXLig+Pl79+vW7Jcf/6KOP5OfnpyeffDLX9Zar5nD+/vvv1atXLw0dOlS7du3S22+/rQULFujFF1+02WbChAl6+OGHtWPHDt1///3q2bOnTp8+XaDnAQBFCcEJAFDoLF++XH5+ftala9euea7z8/PT5MmTc+yjX79+WrBggQzD0KeffqoqVaqofv36t6T+ffv2qXLlyjajW9OmTbOpOTU1VdI/gWjkyJHq3bu3KleurNatW2vixIl6++23bfbZp08fRUdHq2rVqpo8ebLOnz+vTZs23ZLzAYCigFn1AACFTqtWrTRr1izr66ufS/r3OkkqVapUjn20b99ejz/+uNavX6958+bd9GjTE088oQ8//ND6+sKFC2rXrp31FkJJOn/+fJ7b9+vXTx06dNDPP/+sRx55RMb//5TP7du368cff7QZYcrKytLff/+tCxcuyMfHR5JUt25d63pfX18VK1ZMJ06cuKlzAgD8H4ITAKDQ8fX1VdWqVe1edzU3Nzc9+uijGjdunH7++WctWbIk137+/v7W0Z+rnT17VsWLF7e+fuGFF/T0009bX999992aMmWKmjZtmmPbatWq6YcfftClS5fk7u4uSSpRooRKlCiho0eP2vQ9f/68JkyYoAcffDDHfry8vKz/f2U/V1gsFmVnZ+d6TgAA+3GrHpyaxZL/CwBc0a9fP61bt04dO3bMMYHEFTVq1NDWrVtt2rKysrR9+3ZVr17d2lamTBlVrVrVuri5ualcuXI2bVdER0fr/Pnzeuutt0xrbNiwofbu3WuznyuLiws/xgHgVmHECQDwn5KRkaHk5GSbNjc3NwUEBOToW7NmTZ06dcp6u1tuYmNj1b9/f4WHh6t169ZKT0/XjBkzdObMGQ0YMOCGamzWrJlGjBihESNG6PDhw3rwwQcVGhqq48eP691335XFYrGGorFjx+qBBx5QhQoV1KVLF7m4uGj79u3auXOnJk2adEPHBwDYj+AEALDVw3B0BTdlxYoVCgkJsWmrUaOG9uzZk2v/0qVLX3N/0dHRMgxD06ZN08iRI+Xj46OIiAitX79eQUFBN1znK6+8oiZNmmjWrFmaN2+eLly4oKCgILVo0UIbNmxQsWLFJElt2rTR8uXL9cILL2jKlClyd3dXeHj4DYc2AP9hBXVrjVG4fy7kF4thFK0rkZaWpuLFiys1NdX6QwnOqyC+/4vWOx7I299//62DBw+qUqVKNs/KoGj4z379FxXQL46F/A8KjlKofo4XxHvnVr9vCE52sycbcHM0AAAAAJggOAEAAACACZ5xAgAApgrVLVcAUAAYcQIAAAAAEwQnACjiitgcQfj/+LoDgH0ITgBQRLm6ukqSMjMzHVwJHOHChQuSJHd3dwdXAgCFA884AUAR5ebmJh8fH508eVLu7u7WD1zFf5thGLpw4YJOnDihEiVKWAM0AODaCE4AUERZLBaFhITo4MGDOnz4sKPLwS1WokQJBQcHO7oMACg0CE4AUIR5eHioWrVq3K5XxLi7uzPSBAB2IjgBQBHn4uIiLy8vR5cBAIBTIzgBAAAAt1iBfDZa/u8SV+FJYAAAAAAwQXACAAAAABMEJwAAAAAwQXACAAAAABNMDgEAAHATCuQhf57yB5wOI04AAAAAYILgBAAAAAAmCE4AAAAAYILgBAAAAAAmCE4AAAAAYMLhs+q9+eabevnll5WcnKx69eppxowZatKkSZ79p0+frlmzZikpKUkBAQHq0qWL4uLi5OXldQurBv5lUQFMqSRJPZhWCQAAwBk4dMQpPj5esbGxGjdunLZt26Z69eqpTZs2OnHiRK79Fy1apJEjR2rcuHHavXu33n33XcXHx+u55567xZUDAICbZrHk/wIABcShwWnatGkaOHCg+vbtq1q1amn27Nny8fHRvHnzcu3/008/6c4771SPHj0UFham++67T9HR0dq0adMtrhwAAABAUeKw4JSZmamtW7cqMjLy/4pxcVFkZKQ2bNiQ6zZ33HGHtm7dag1Kf/zxh77++mvdf//9eR4nIyNDaWlpNgsAAAAA2MNhzzidOnVKWVlZCgoKsmkPCgrSnj17ct2mR48eOnXqlO666y4ZhqHLly/riSeeuOatenFxcZowYUK+1g4AAACgaClUs+qtXbtWkydP1ltvvaVt27bp888/11dffaWJEyfmuc2oUaOUmppqXY4cOXILKwYAAADwX+CwEaeAgAC5uroqJSXFpj0lJUXBwcG5bjNmzBg9+uijGjBggCTptttuU3p6uh577DGNHj1aLi45c6Cnp6c8PT3z/wQAAAAAFBkOG3Hy8PBQRESEEhISrG3Z2dlKSEhQs2bNct3mwoULOcKRq6urJMkwmLYZAAAAQMFw6Oc4xcbGqnfv3mrUqJGaNGmi6dOnKz09XX379pUk9erVS+XKlVNcXJwkKSoqStOmTVODBg3UtGlT7d+/X2PGjFFUVJQ1QAEAAABAfnNocOrWrZtOnjypsWPHKjk5WfXr19eKFSusE0YkJSXZjDA9//zzslgsev7553Xs2DEFBgYqKipKL774oqNOAQAAAEARYDGK2D1uaWlpKl68uFJTU1WsWDFHlwMTBfFZhgXyjl9UQB+62KNIfXsCcGIF8u+xCmCnC/N/l5Ku+e9xoflZ5QCF6toUxM/yW/2+KYjvKem/84bMhT3ZoFDNqgcAAAAAjkBwAgAAAAATBCcAAAAAMEFwAgAAQOFhsRTMApggOAEAAACACYdORw4AuLUK6o+q/+EJlwAAkMSIEwAAAACYIjgBAAAAgAmCEwAAAACYIDgBAAAAgAmCEwAAAACYIDgBAAAAgAmCEwAAAACYIDgBAAAAgAmCEwAAAACYcHN0AQAAAACc2CJL/u+zh5H/+yxgjDgBAAAAgAmCEwAAAACYIDgBAAAAgAmCEwAAAACYIDgBAAAAgAmCEwAAAACYIDgBAAAAgAmCEwAAAACYIDgBAAAAgAmCEwAAAACYIDgBAAAAgAmCEwAAAACYIDgBAAAAgAmCEwAAAACYIDgBAAAAgAmCEwAAAACYIDgBAAAAgAmCEwAAAACYIDgBAAAAgAmCEwAAAACYIDih6LFY8n8BAADAfxrBCQAAAABMEJwAAAAAwATBCQAAAABMEJwAAAAAwISbowsAAOA/rSAmkDGM/N8nAOCaCE4AAKjgJsgk4gDAfwO36gEAAACACYITAAAAAJggOAEAAACACZ5xAgAAcDZMKgI4HUacAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATBCcAAAAAMAEwQkAAAAATLg5ugAAyG8WS/7v0zDyf58AAKDwYMQJAAAAAEwQnAAAAADABMEJAAAAAEwQnAAAAADABMEJAAAAAEwQnAAAAADABMEJAAAAAEwQnAAAAADABMEJAAAAAEwQnAAAAADABMEJAAAAAEwQnAAAAADABMEJAAAAAEwQnAAAAADABMEJAAAAAEwQnAAAAADABMEJAAAAAEy4OboAAAAA3AKLLAWz3x5GwewXcDI3NeL0999/51cdAAAAAOC07B5xys7O1osvvqjZs2crJSVF+/btU+XKlTVmzBiFhYWpf//+BVEnAKCoKYi/jvOXcQDADbJ7xGnSpElasGCBpk6dKg8PD2t7nTp19M4779hdwJtvvqmwsDB5eXmpadOm2rRp0zX7nz17VjExMQoJCZGnp6eqV6+ur7/+2u7jAgAAAMD1sjs4vf/++5ozZ4569uwpV1dXa3u9evW0Z88eu/YVHx+v2NhYjRs3Ttu2bVO9evXUpk0bnThxItf+mZmZat26tQ4dOqRPP/1Ue/fu1dy5c1WuXDl7TwMAAAAArpvdt+odO3ZMVatWzdGenZ2tS5cu2bWvadOmaeDAgerbt68kafbs2frqq680b948jRw5Mkf/efPm6fTp0/rpp5/k7u4uSQoLC7P3FAAAAADALnaPONWqVUvff/99jvZPP/1UDRo0uO79ZGZmauvWrYqMjPy/YlxcFBkZqQ0bNuS6zbJly9SsWTPFxMQoKChIderU0eTJk5WVlZXncTIyMpSWlmazAAAAAIA97B5xGjt2rHr37q1jx44pOztbn3/+ufbu3av3339fy5cvv+79nDp1SllZWQoKCrJpDwoKyvOWvz/++EPfffedevbsqa+//lr79+/Xk08+qUuXLmncuHG5bhMXF6cJEyZc/wkCAAAAwL/YPeLUsWNHffnll1q9erV8fX01duxY7d69W19++aVat25dEDVaZWdnq0yZMpozZ44iIiLUrVs3jR49WrNnz85zm1GjRik1NdW6HDlypEBrBAAAAPDfY9eI0+XLlzV58mT169dPq1atuqkDBwQEyNXVVSkpKTbtKSkpCg4OznWbkJAQubu720xKUbNmTSUnJyszM9Nmlr8rPD095enpeVO1AgAAACja7BpxcnNz09SpU3X58uWbPrCHh4ciIiKUkJBgbcvOzlZCQoKaNWuW6zZ33nmn9u/fr+zsbGvbvn37FBISkmtoAgAAAID8YPetevfee6/WrVuXLwePjY3V3Llz9d5772n37t0aNGiQ0tPTrbPs9erVS6NGjbL2HzRokE6fPq2hQ4dq3759+uqrrzR58mTFxMTkSz0AAAAAkBu7J4do166dRo4cqV9//VURERHy9fW1Wd+hQ4fr3le3bt108uRJjR07VsnJyapfv75WrFhhnTAiKSlJLi7/l+1CQ0O1cuVKDR8+XHXr1lW5cuU0dOhQPfvss/aeBgAAAABcN4thGIY9G1wdZHLszGK55tTgziAtLU3FixdXamqqihUr5uhyYMJiyf99GiqAnS7M/11KknrY9e2J/6/QvG8kyb5/gm9aQVwbqYBOY1EBFHuN76kCuzYF8d65xe8bqRB9Xzng32OuzX/g2kgFc324Nnlzkt9x7MkGdo84Xf18EQAAAAAUBXY/4wQAAAAARc0NBad169YpKipKVatWVdWqVdWhQwd9//33+V0bAAAAADgFu4PThx9+qMjISPn4+GjIkCEaMmSIvL29de+992rRokUFUSMAAAAAOJTdzzi9+OKLmjp1qoYPH25tGzJkiKZNm6aJEyeqR48e+VogAAAAADia3SNOf/zxh6KionK0d+jQQQcPHsyXogAAAADAmdg94hQaGqqEhARVrVrVpn316tUKDQ3Nt8IAoEi4xVNuAwCAG2N3cBoxYoSGDBmixMRE3XHHHZKkH3/8UQsWLNDrr7+e7wUCAIB/IXADwC1nd3AaNGiQgoOD9eqrr+rjjz+WJNWsWVPx8fHq2LFjvhcIAAAAAI5md3CSpM6dO6tz5875XQsAAAAAOCW7J4fYvHmzfv755xztP//8s7Zs2ZIvRQEAChmLJf8XAACciN3BKSYmRkeOHMnRfuzYMcXExORLUQAAAADgTOwOTrt27VLDhg1ztDdo0EC7du3Kl6IAAAAAwJnYHZw8PT2VkpKSo/348eNyc7uhR6YAAAAAwKnZHZzuu+8+jRo1Sqmpqda2s2fP6rnnnlPr1q3ztTgAAAAAcAZ2DxG98soratGihSpWrKgGDRpIkhITExUUFKQPPvgg3wsEAAAAAEezOziVK1dOO3bs0MKFC7V9+3Z5e3urb9++io6Olru7e0HUCAAAAAAOdUMPJfn6+uqxxx7L71oAAAAAwCld9zNO+/bt06ZNm2zaEhIS1KpVKzVp0kSTJ0/O9+IAAAAAwBlcd3B69tlntXz5cuvrgwcPKioqSh4eHmrWrJni4uI0ffr0gqgRAAAAABzqum/V27Jli/73v/9ZXy9cuFDVq1fXypUrJUl169bVjBkzNGzYsHwvEgAAAAAc6bpHnE6dOqXy5ctbX69Zs0ZRUVHW13fffbcOHTqUr8UBAAAAgDO47uBUqlQpHT9+XJKUnZ2tLVu26Pbbb7euz8zMlGEY+V8hAAAAADjYdQenu+++WxMnTtSRI0c0ffp0ZWdn6+6777au37Vrl8LCwgqgRAAAAABwrOt+xunFF19U69atVbFiRbm6uuqNN96Qr6+vdf0HH3yge+65p0CKBAAAAABHuu7gFBYWpt27d+u3335TYGCgypYta7N+woQJNs9AAQAAAMB/hV0fgOvm5qZ69erlui6vdgAAAAAo7K77GScAAAAAKKoITgAAAABgguAEAAAAACYITgAAAABgwu7gFBYWphdeeEFJSUkFUQ8AAAAAOB27g9OwYcP0+eefq3LlymrdurUWL16sjIyMgqgNAAAAAJzCDQWnxMREbdq0STVr1tRTTz2lkJAQDR48WNu2bSuIGgEAAADAoW74GaeGDRvqjTfe0J9//qlx48bpnXfeUePGjVW/fn3NmzdPhmHkZ50AAAAA4DB2fQDu1S5duqQlS5Zo/vz5WrVqlW6//Xb1799fR48e1XPPPafVq1dr0aJF+VkrAAAAADiE3cFp27Ztmj9/vj766CO5uLioV69eeu211xQeHm7t07lzZzVu3DhfCwUAAAAAR7E7ODVu3FitW7fWrFmz1KlTJ7m7u+foU6lSJXXv3j1fCgQAAAAAR7M7OP3xxx+qWLHiNfv4+vpq/vz5N1wUAAAAADgTuyeHOHHihH7++ecc7T///LO2bNmSL0UBAAAAgDOxOzjFxMToyJEjOdqPHTummJiYfCkKAAAAAJyJ3cFp165datiwYY72Bg0aaNeuXflSFAAAAAA4E7uDk6enp1JSUnK0Hz9+XG5uNzy7OQAAAAA4LbuD03333adRo0YpNTXV2nb27Fk999xzat26db4WBwAAAADOwO4holdeeUUtWrRQxYoV1aBBA0lSYmKigoKC9MEHH+R7gQAAAADgaHYHp3LlymnHjh1auHChtm/fLm9vb/Xt21fR0dG5fqYTAAAAABR2N/RQkq+vrx577LH8rgUAAAAAnNINz+awa9cuJSUlKTMz06a9Q4cON10UAAAAADgTu4PTH3/8oc6dO+vXX3+VxWKRYRiSJIvFIknKysrK3woBAAAAwMHsnlVv6NChqlSpkk6cOCEfHx/99ttvWr9+vRo1aqS1a9cWQIkAAAAA4Fh2jzht2LBB3333nQICAuTi4iIXFxfdddddiouL05AhQ/TLL78URJ0AAAAA4DB2jzhlZWXJ399fkhQQEKA///xTklSxYkXt3bs3f6sDAAAAACdg94hTnTp1tH37dlWqVElNmzbV1KlT5eHhoTlz5qhy5coFUSMAAAAAOJTdwen5559Xenq6JOmFF17QAw88oObNm6t06dKKj4/P9wIBAAAAwNHsDk5t2rSx/n/VqlW1Z88enT59WiVLlrTOrAcAAAAA/yV2PeN06dIlubm5aefOnTbtpUqVIjQBAAAA+M+yKzi5u7urQoUKfFYTAAAAgCLF7ln1Ro8ereeee06nT58uiHoAAAAAwOnY/YzTzJkztX//fpUtW1YVK1aUr6+vzfpt27blW3EAAAAA4AzsDk6dOnUqgDIAAAAAwHnZHZzGjRtXEHUAAAAAgNOy+xknAAAAAChq7B5xcnFxuebU48y4BwAAAOC/xu7gtGTJEpvXly5d0i+//KL33ntPEyZMyLfCAAAAAMBZ2B2cOnbsmKOtS5cuql27tuLj49W/f/98KQwAAAAAnEW+PeN0++23KyEhIb92BwAAAABOI1+C08WLF/XGG2+oXLly+bE7AAAAAHAqdt+qV7JkSZvJIQzD0Llz5+Tj46MPP/wwX4sDAAAAAGdgd3B67bXXbIKTi4uLAgMD1bRpU5UsWTJfiwMAAAAAZ2B3cOrTp08BlAEAAAAAzsvuZ5zmz5+vTz75JEf7J598ovfeey9figIAAAAAZ2J3cIqLi1NAQECO9jJlymjy5Mn5UhQAAAAAOBO7g1NSUpIqVaqUo71ixYpKSkrKl6IAAAAAwJnYHZzKlCmjHTt25Gjfvn27SpcunS9FAQAAAIAzsTs4RUdHa8iQIVqzZo2ysrKUlZWl7777TkOHDlX37t0LokYAAAAAcCi7Z9WbOHGiDh06pHvvvVdubv9snp2drV69evGMEwAAAID/JLuDk4eHh+Lj4zVp0iQlJibK29tbt912mypWrFgQ9QEAAACAw9kdnK6oVq2aqlWrlp+1AAAAAIBTsvsZp4ceekhTpkzJ0T516lR17do1X4oCAAAAAGdid3Bav3697r///hzt7dq10/r16/OlKAAAAABwJnYHp/Pnz8vDwyNHu7u7u9LS0vKlKAAAAABwJnYHp9tuu03x8fE52hcvXqxatWrdUBFvvvmmwsLC5OXlpaZNm2rTpk3Xtd3ixYtlsVjUqVOnGzouAAAAAFwPuyeHGDNmjB588EEdOHBA99xzjyQpISFBH330kT755BO7C4iPj1dsbKxmz56tpk2bavr06WrTpo327t2rMmXK5LndoUOH9PTTT6t58+Z2HxMAAAAA7GH3iFNUVJSWLl2q/fv368knn9SIESN09OhRrV69+oZGfqZNm6aBAweqb9++qlWrlmbPni0fHx/Nmzcvz22ysrLUs2dPTZgwQZUrV7b7mAAAAABgjxuajrx9+/Zq3759jvadO3eqTp06172fzMxMbd26VaNGjbK2ubi4KDIyUhs2bMhzuxdeeEFlypRR//799f3331/zGBkZGcrIyLC+5jksAAAAAPaye8Tp386dO6c5c+aoSZMmqlevnl3bnjp1SllZWQoKCrJpDwoKUnJycq7b/PDDD3r33Xc1d+7c6zpGXFycihcvbl1CQ0PtqhEAAAAAbjg4rV+/Xr169VJISIheeeUV3XPPPdq4cWN+1pbDuXPn9Oijj2ru3LkKCAi4rm1GjRql1NRU63LkyJECrREAAADAf49dt+olJydrwYIFevfdd5WWlqaHH35YGRkZWrp06Q3NqBcQECBXV1elpKTYtKekpCg4ODhH/wMHDujQoUOKioqytmVnZ/9zIm5u2rt3r6pUqWKzjaenpzw9Pe2uDQAAAACuuO4Rp6ioKNWoUUM7duzQ9OnT9eeff2rGjBk3dXAPDw9FREQoISHB2padna2EhAQ1a9YsR//w8HD9+uuvSkxMtC4dOnRQq1atlJiYyG14AAAAAArEdY84ffPNNxoyZIgGDRqkatWq5VsBsbGx6t27txo1aqQmTZpo+vTpSk9PV9++fSVJvXr1Urly5RQXFycvL68ck0+UKFFCkuyalAIAAAAA7HHdwenKpAwRERGqWbOmHn30UXXv3v2mC+jWrZtOnjypsWPHKjk5WfXr19eKFSusE0YkJSXJxeWm57AAAAAAgBt23cHp9ttv1+23367p06crPj5e8+bNU2xsrLKzs7Vq1SqFhobK39//hooYPHiwBg8enOu6tWvXXnPbBQsW3NAxAQAAAOB62T2U4+vrq379+umHH37Qr7/+qhEjRuill15SmTJl1KFDh4KoEQAAAAAc6qbugatRo4amTp2qo0eP6qOPPsqvmgAAAADAqeTLw0Ourq7q1KmTli1blh+7AwAAAACnwqwLAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJghOAAAAAGCC4AQAAAAAJpwiOL355psKCwuTl5eXmjZtqk2bNuXZd+7cuWrevLlKliypkiVLKjIy8pr9AQAAAOBmOTw4xcfHKzY2VuPGjdO2bdtUr149tWnTRidOnMi1/9q1axUdHa01a9Zow4YNCg0N1X333adjx47d4soBAAAAFBUOD07Tpk3TwIED1bdvX9WqVUuzZ8+Wj4+P5s2bl2v/hQsX6sknn1T9+vUVHh6ud955R9nZ2UpISLjFlQMAAAAoKhwanDIzM7V161ZFRkZa21xcXBQZGakNGzZc1z4uXLigS5cuqVSpUrmuz8jIUFpams0CAAAAAPZwaHA6deqUsrKyFBQUZNMeFBSk5OTk69rHs88+q7Jly9qEr6vFxcWpePHi1iU0NPSm6wYAAABQtDj8Vr2b8dJLL2nx4sVasmSJvLy8cu0zatQopaamWpcjR47c4ioBAAAAFHZujjx4QECAXF1dlZKSYtOekpKi4ODga277yiuv6KWXXtLq1atVt27dPPt5enrK09MzX+oFAAAAUDQ5dMTJw8NDERERNhM7XJnooVmzZnluN3XqVE2cOFErVqxQo0aNbkWpAAAAAIowh444SVJsbKx69+6tRo0aqUmTJpo+fbrS09PVt29fSVKvXr1Urlw5xcXFSZKmTJmisWPHatGiRQoLC7M+C+Xn5yc/Pz+HnQcAAACA/y6HB6du3brp5MmTGjt2rJKTk1W/fn2tWLHCOmFEUlKSXFz+b2Bs1qxZyszMVJcuXWz2M27cOI0fP/5Wlg4AAACgiHB4cJKkwYMHa/DgwbmuW7t2rc3rQ4cOFXxBAAAAAHCVQj2rHgAAAADcCgQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADBBcAIAAAAAEwQnAAAAADDhFMHpzTffVFhYmLy8vNS0aVNt2rTpmv0/+eQThYeHy8vLS7fddpu+/vrrW1QpAAAAgKLI4cEpPj5esbGxGjdunLZt26Z69eqpTZs2OnHiRK79f/rpJ0VHR6t///765Zdf1KlTJ3Xq1Ek7d+68xZUDAAAAKCocHpymTZumgQMHqm/fvqpVq5Zmz54tHx8fzZs3L9f+r7/+utq2batnnnlGNWvW1MSJE9WwYUPNnDnzFlcOAAAAoKhwc+TBMzMztXXrVo0aNcra5uLiosjISG3YsCHXbTZs2KDY2FibtjZt2mjp0qW59s/IyFBGRob1dWpqqiQpLS3tJqtHYVUgX/kLBbFTSbxPnUaBfSUK4r3jgPdNofm+4trkjWtzbbf4+nBtrnG4gtrxf+D7imtjvyuZwDAM886GAx07dsyQZPz000827c8884zRpEmTXLdxd3c3Fi1aZNP25ptvGmXKlMm1/7hx4wxJLCwsLCwsLCwsLCwsuS5HjhwxzS4OHXG6FUaNGmUzQpWdna3Tp0+rdOnSslgsDqzMOaSlpSk0NFRHjhxRsWLFHF2OU+Ha5I1rkzeuTd64Nnnj2lwb1ydvXJu8cW3yxrX5P4Zh6Ny5cypbtqxpX4cGp4CAALm6uiolJcWmPSUlRcHBwbluExwcbFd/T09PeXp62rSVKFHixov+jypWrFiR/8bJC9cmb1ybvHFt8sa1yRvX5tq4Pnnj2uSNa5M3rs0/ihcvfl39HDo5hIeHhyIiIpSQkGBty87OVkJCgpo1a5brNs2aNbPpL0mrVq3Ksz8AAAAA3CyH36oXGxur3r17q1GjRmrSpImmT5+u9PR09e3bV5LUq1cvlStXTnFxcZKkoUOHqmXLlnr11VfVvn17LV68WFu2bNGcOXMceRoAAAAA/sMcHpy6deumkydPauzYsUpOTlb9+vW1YsUKBQUFSZKSkpLk4vJ/A2N33HGHFi1apOeff17PPfecqlWrpqVLl6pOnTqOOoVCzdPTU+PGjctxOyO4NtfCtckb1yZvXJu8cW2ujeuTN65N3rg2eePa3BiLYVzP3HsAAAAAUHQ5/ANwAQAAAMDZEZwAAAAAwATBCQAAAABMEJwAAAAAwATBqQh78803FRYWJi8vLzVt2lSbNm1ydElOYf369YqKilLZsmVlsVi0dOlSR5fkFOLi4tS4cWP5+/urTJky6tSpk/bu3evospzGrFmzVLduXeuHCTZr1kzffPONo8tySi+99JIsFouGDRvm6FIcbvz48bJYLDZLeHi4o8tyGseOHdMjjzyi0qVLy9vbW7fddpu2bNni6LIcLiwsLMf7xmKxKCYmxtGlOVxWVpbGjBmjSpUqydvbW1WqVNHEiRPFXGj/OHfunIYNG6aKFSvK29tbd9xxhzZv3uzosgoNglMRFR8fr9jYWI0bN07btm1TvXr11KZNG504ccLRpTlcenq66tWrpzfffNPRpTiVdevWKSYmRhs3btSqVat06dIl3XfffUpPT3d0aU6hfPnyeumll7R161Zt2bJF99xzjzp27KjffvvN0aU5lc2bN+vtt99W3bp1HV2K06hdu7aOHz9uXX744QdHl+QUzpw5ozvvvFPu7u765ptvtGvXLr366qsqWbKko0tzuM2bN9u8Z1atWiVJ6tq1q4Mrc7wpU6Zo1qxZmjlzpnbv3q0pU6Zo6tSpmjFjhqNLcwoDBgzQqlWr9MEHH+jXX3/Vfffdp8jISB07dszRpRUKTEdeRDVt2lSNGzfWzJkzJUnZ2dkKDQ3VU089pZEjRzq4OudhsVi0ZMkSderUydGlOJ2TJ0+qTJkyWrdunVq0aOHocpxSqVKl9PLLL6t///6OLsUpnD9/Xg0bNtRbb72lSZMmqX79+po+fbqjy3Ko8ePHa+nSpUpMTHR0KU5n5MiR+vHHH/X99987uhSnN2zYMC1fvly///67LBaLo8txqAceeEBBQUF69913rW0PPfSQvL299eGHHzqwMse7ePGi/P399cUXX6h9+/bW9oiICLVr106TJk1yYHWFAyNORVBmZqa2bt2qyMhIa5uLi4siIyO1YcMGB1aGwiQ1NVXSP+EAtrKysrR48WKlp6erWbNmji7HacTExKh9+/Y2//ZA+v3331W2bFlVrlxZPXv2VFJSkqNLcgrLli1To0aN1LVrV5UpU0YNGjTQ3LlzHV2W08nMzNSHH36ofv36FfnQJEl33HGHEhIStG/fPknS9u3b9cMPP6hdu3YOrszxLl++rKysLHl5edm0e3t7M9J9ndwcXQBuvVOnTikrK0tBQUE27UFBQdqzZ4+DqkJhkp2drWHDhunOO+9UnTp1HF2O0/j111/VrFkz/f333/Lz89OSJUtUq1YtR5flFBYvXqxt27ZxL/2/NG3aVAsWLFCNGjV0/PhxTZgwQc2bN9fOnTvl7+/v6PIc6o8//tCsWbMUGxur5557Tps3b9aQIUPk4eGh3r17O7o8p7F06VKdPXtWffr0cXQpTmHkyJFKS0tTeHi4XF1dlZWVpRdffFE9e/Z0dGkO5+/vr2bNmmnixImqWbOmgoKC9NFHH2nDhg2qWrWqo8srFAhOAOwWExOjnTt38heqf6lRo4YSExOVmpqqTz/9VL1799a6deuKfHg6cuSIhg4dqlWrVuX4S2dRd/VfwevWraumTZuqYsWK+vjjj4v8LZ7Z2dlq1KiRJk+eLElq0KCBdu7cqdmzZxOcrvLuu++qXbt2Klu2rKNLcQoff/yxFi5cqEWLFql27dpKTEzUsGHDVLZsWd43kj744AP169dP5cqVk6urqxo2bKjo6Ght3brV0aUVCgSnIiggIECurq5KSUmxaU9JSVFwcLCDqkJhMXjwYC1fvlzr169X+fLlHV2OU/Hw8LD+1S4iIkKbN2/W66+/rrffftvBlTnW1q1bdeLECTVs2NDalpWVpfXr12vmzJnKyMiQq6urAyt0HiVKlFD16tW1f/9+R5ficCEhITn+6FCzZk199tlnDqrI+Rw+fFirV6/W559/7uhSnMYzzzyjkSNHqnv37pKk2267TYcPH1ZcXBzBSVKVKlW0bt06paenKy0tTSEhIerWrZsqV67s6NIKBZ5xKoI8PDwUERGhhIQEa1t2drYSEhJ4HgN5MgxDgwcP1pIlS/Tdd9+pUqVKji7J6WVnZysjI8PRZTjcvffeq19//VWJiYnWpVGjRurZs6cSExMJTVc5f/68Dhw4oJCQEEeX4nB33nlnjo882LdvnypWrOigipzP/PnzVaZMGZsH/Yu6CxcuyMXF9tdbV1dXZWdnO6gi5+Tr66uQkBCdOXNGK1euVMeOHR1dUqHAiFMRFRsbq969e6tRo0Zq0qSJpk+frvT0dPXt29fRpTnc+fPnbf7ae/DgQSUmJqpUqVKqUKGCAytzrJiYGC1atEhffPGF/P39lZycLEkqXry4vL29HVyd440aNUrt2rVThQoVdO7cOS1atEhr167VypUrHV2aw/n7++d4Fs7X11elS5cu8s/IPf3004qKilLFihX1559/aty4cXJ1dVV0dLSjS3O44cOH64477tDkyZP18MMPa9OmTZozZ47mzJnj6NKcQnZ2tubPn6/evXvLzY1f566IiorSiy++qAoVKqh27dr65ZdfNG3aNPXr18/RpTmFlStXyjAM1ahRQ/v379czzzyj8PBwfv+7XgaKrBkzZhgVKlQwPDw8jCZNmhgbN250dElOYc2aNYakHEvv3r0dXZpD5XZNJBnz5893dGlOoV+/fkbFihUNDw8PIzAw0Lj33nuNb7/91tFlOa2WLVsaQ4cOdXQZDtetWzcjJCTE8PDwMMqVK2d069bN2L9/v6PLchpffvmlUadOHcPT09MIDw835syZ4+iSnMbKlSsNScbevXsdXYpTSUtLM4YOHWpUqFDB8PLyMipXrmyMHj3ayMjIcHRpTiE+Pt6oXLmy4eHhYQQHBxsxMTHG2bNnHV1WocHnOAEAAACACZ5xAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgAAAAATBCcAAAAAMEFwAgD856xdu1YWi0Vnz551dCkAgP8IghMAwOmcPHlSgwYNUoUKFeTp6ang4GC1adNGP/74o6NLs7r77rs1bNgwR5cBALhF3BxdAAAA//bQQw8pMzNT7733nipXrqyUlBQlJCTor7/+cnRpAIAiihEnAIBTOXv2rL7//ntNmTJFrVq1UsWKFdWkSRONGjVKHTp00KFDh2SxWJSYmGizjcVi0dq1a2329eOPP6pu3bry8vLS7bffrp07d1rXHT58WFFRUSpZsqR8fX1Vu3Ztff3119b1O3fuVLt27eTn56egoCA9+uijOnXqlCSpT58+WrdunV5//XVZLBZZLBYdOnRIZ86cUc+ePRUYGChvb29Vq1ZN8+fPL9DrBQC4NQhOAACn4ufnJz8/Py1dulQZGRk3ta9nnnlGr776qjZv3qzAwEBFRUXp0qVLkqSYmBhlZGRo/fr1+vXXXzVlyhT5+flJ+ieI3XPPPWrQoIG2bNmiFStWKCUlRQ8//LAk6fXXX1ezZs00cOBAHT9+XMePH1doaKjGjBmjXbt26ZtvvtHu3bs1a9YsBQQE3NwFAQA4BW7VAwA4FTc3Ny1YsEADBw7U7Nmz1bBhQ7Vs2VLdu3dX3bp17drXuHHj1Lp1a0nSe++9p/Lly2vJkiV6+OGHlZSUpIceeki33XabJKly5crW7WbOnKkGDRpo8uTJ1rZ58+YpNDRU+/btU/Xq1eXh4SEfHx8FBwdb+yQlJalBgwZq1KiRJCksLOxGLwMAwMkw4gQAcDoPPfSQ/vzzTy1btkxt27bV2rVr1bBhQy1YsMCu/TRr1sz6/6VKlVKNGjW0e/duSdKQIUM0adIk3XnnnRo3bpx27Nhh7bt9+3atWbPGOvrl5+en8PBwSdKBAwfyPN6gQYO0ePFi1a9fX//73//0008/2VUvAMB5EZwAAE7Jy8tLrVu31pgxY/TTTz+pT58+GjdunFxc/vnRZRiGte+V2+/sMWDAAP3xxx969NFH9euvv6pRo0aaMWOGJOn8+fOKiopSYmKizfL777+rRYsWee6zXbt2Onz4sIYPH64///xT9957r55++mm7awMAOB+CEwCgUKhVq5bS09MVGBgoSTp+/Lh13dUTRVxt48aN1v8/c+aM9u3bp5o1a1rbQkND9cQTT+jzzz/XiBEjNHfuXElSw4YN9dtvvyksLExVq1a1WXx9fSVJHh4eysrKynHMwMBA9e7dWx9++KGmT5+uOXPm3PS5AwAcj2ecAABO5a+//lLXrl3Vr18/1a1bV/7+/tqyZYumTp2qjh07ytvbW7fffrteeuklVapUSSdOnNDzzz+f675eeOEFlS5dWkFBQRo9erQCAgLUqVMnSdKwYcPUrl07Va9eXWfOnNGaNWusoSomJkZz585VdHS0/ve//6lUqVLav3+/Fi9erHfeeUeurq4KCwvTzz//rEOHDsnPz0+lSpXS+PHjFRERodq1aysjI0PLly+3CWoAgMKLEScAgFPx8/NT06ZN9dprr6lFixaqU6eOxowZo4EDB2rmzJmS/pmo4fLly4qIiNCwYcM0adKkXPf10ksvaejQoYqIiFBycrK+/PJLeXh4SJKysrIUExOjmjVrqm3btqpevbreeustSVLZsmX1448/KisrS/fdd59uu+02DRs2TCVKlLDeKvj000/L1dVVtWrVUmBgoJKSkuTh4aFRo0apbt26atGihVxdXbV48eJbcNUAAAXNYlx9kzgAAAAAIAdGnAAAAADABMEJAAAAAEwQnAAAAADABMEJAAAAAEwQnAAAAADABMEJAAAAAEwQnAAAAADABMEJAAAAAEwQnAAAAADABMEJAAAAAEwQnAAAAADAxP8D9AQhiq+m6JwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_boxplots(class_accs_exact, class_accs_paper, class_accs_gemu, \"Accuracy Comparison of Models\", \"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "974870e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7518858313560486)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delta Acc\n",
    "\n",
    "sum([np.absolute(sum(i)/len(i)-(sum(j)/len(j))) for i, j in zip(class_accs_exact.values(), class_accs_paper.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea719ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7669382691383362)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sum([np.absolute(sum(i)/len(i)-(sum(j)/len(j))) for i, j in zip(class_accs_exact.values(), class_accs_gemu.values())])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bach.conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
